# üéØ ROADMAP REORDENADO: Pre-Hardware Focus
## Estrategia: "Perfect the Simulation" mientras llega el EEG

> **Filosof√≠a**: Construir el sistema completo con datos simulados de forma tan robusta que integrar hardware real sea solo "cambiar la fuente de datos".

---

## üìç SITUACI√ìN ACTUAL

### ‚úÖ Fortalezas
- VAE entrenado con PhysioNet (109 sujetos)
- WebSocket streaming funcional (5Hz)
- Visualizaci√≥n 3D reactiva con Three.js
- Shaders b√°sicos sint√©rgicos

### üî¥ Gaps (Sin necesidad de hardware)
- M√©tricas sint√©rgicas superficiales
- Sin an√°lisis espectral (FFT, bandas)
- Shaders b√°sicos (sin efectos cu√°nticos)
- UI/UX m√≠nima (sin gr√°ficas)
- Sin audio feedback (binaural beats)
- Sin modularizaci√≥n del c√≥digo

---

## üöÄ FASE 1: BACKEND - An√°lisis Cient√≠fico (3-5 d√≠as)

**Objetivo**: Implementar m√©tricas sint√©rgicas reales aunque sea con datos simulados.

### 1.1 M√≥dulo de An√°lisis Espectral

```python
# backend/analysis/spectral.py
class SpectralAnalyzer:
    """FFT y an√°lisis de bandas de frecuencia"""
    
    @staticmethod
    def compute_frequency_bands(eeg_signal, fs=256):
        """
        Retorna potencia en cada banda:
        - Delta (0.5-4 Hz): Sue√±o profundo
        - Theta (4-8 Hz): Meditaci√≥n profunda, creatividad
        - Alpha (8-13 Hz): Relajaci√≥n, coherencia
        - Beta (13-30 Hz): Concentraci√≥n, alerta
        - Gamma (30-50 Hz): Insight, procesamiento cognitivo alto
        """
        pass
```

**Por qu√© AHORA**: 
- Funciona perfectamente con dataset PhysioNet
- No requiere hardware nuevo
- Mejora dr√°sticamente las m√©tricas que mandamos al frontend
- **Bloqueador**: Frontend necesita estas m√©tricas para visualizaciones ricas

### 1.2 Coherencia Inter-Hemisf√©rica Real

```python
# backend/analysis/coherence.py
class CoherenceAnalyzer:
    @staticmethod
    def inter_hemispheric_coherence(left_channels, right_channels, freq_band=(8, 13)):
        """
        Coherencia REAL usando Phase Locking Value (PLV)
        No solo "1/varianza", sino correlaci√≥n de fases en Alpha
        """
        pass
```

**Por qu√© AHORA**:
- Dataset PhysioNet tiene m√∫ltiples canales L/R
- M√©trica core de la teor√≠a sint√©rgica
- Validable con papers cient√≠ficos

### 1.3 Refactorizaci√≥n Modular

**Estructura objetivo**:
```
backend/
‚îú‚îÄ‚îÄ analysis/          # ‚Üê NUEVO
‚îÇ   ‚îú‚îÄ‚îÄ spectral.py
‚îÇ   ‚îú‚îÄ‚îÄ coherence.py
‚îÇ   ‚îú‚îÄ‚îÄ entropy.py
‚îÇ   ‚îî‚îÄ‚îÄ metrics.py    # Orquestador
‚îú‚îÄ‚îÄ ai/
‚îÇ   ‚îú‚îÄ‚îÄ inference.py  # ‚Üê Refactorizar
‚îÇ   ‚îî‚îÄ‚îÄ model.py
‚îî‚îÄ‚îÄ main.py          # ‚Üê M√°s limpio
```

**Beneficios**:
- C√≥digo testeable (unit tests)
- F√°cil agregar hardware despu√©s
- Separaci√≥n de concerns

**Tareas**:
- [ ] Crear m√≥dulo `analysis/`
- [ ] Mover l√≥gica de m√©tricas de `inference.py` a `metrics.py`
- [ ] Actualizar `main.py` para usar nuevo m√≥dulo
- [ ] Tests b√°sicos

---

## üé® FASE 2: FRONTEND - UX/UI Cient√≠fica (4-6 d√≠as)

**Objetivo**: Dashboard profesional que muestre TODAS las m√©tricas en tiempo real.

### 2.1 Componentes de Visualizaci√≥n

```jsx
frontend/src/components/hud/
‚îú‚îÄ‚îÄ CoherenceMeter.jsx       # Gr√°fica en tiempo real
‚îú‚îÄ‚îÄ FrequencySpectrum.jsx    # Barras de Delta, Theta, Alpha, Beta, Gamma
‚îú‚îÄ‚îÄ StateIndicator.jsx       # "MEDITATING" / "FOCUSED" / "RELAXED"
‚îî‚îÄ‚îÄ SessionStats.jsx         # Duraci√≥n, promedio coherencia
```

**Mockups conceptuales**:

#### CoherenceMeter
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ COHERENCE                   ‚îÇ
‚îÇ ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë 0.73    ‚îÇ
‚îÇ                             ‚îÇ
‚îÇ [Graph showing last 30s]    ‚îÇ
‚îÇ     ‚ï±‚ï≤    ‚ï±‚ï≤                ‚îÇ
‚îÇ    ‚ï±  ‚ï≤  ‚ï±  ‚ï≤               ‚îÇ
‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 0.5   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

#### FrequencySpectrum
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ FREQUENCY BANDS             ‚îÇ
‚îÇ                             ‚îÇ
‚îÇ Œ¥ ‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë  0.2  (Sleep)  ‚îÇ
‚îÇ Œ∏ ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë  0.4  (Deep)   ‚îÇ
‚îÇ Œ± ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  0.8  (Relax)  ‚îÇ ‚Üê Dominante
‚îÇ Œ≤ ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë  0.3  (Focus)  ‚îÇ
‚îÇ Œ≥ ‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë  0.1  (Insight)‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Por qu√© AHORA**:
- Mejora UX inmediatamente
- No depende de hardware (funciona con datos simulados)
- Necesario para neurofeedback despu√©s
- **Bloqueador para demos**: Sin esto, es solo "un cerebro bonito"

### 2.2 Audio Binaural Reactivo

```javascript
// frontend/src/hooks/useAudioFeedback.js
export const useAudioFeedback = () => {
  const { coherence } = useBrainStore()
  
  useEffect(() => {
    // Frecuencia base ajustada por coherencia
    const carrierFreq = 200 + (coherence * 100) // 200-300 Hz
    
    // Binaural beat en rango Alpha (10 Hz diferencia)
    const leftFreq = carrierFreq
    const rightFreq = carrierFreq + 10 // Alpha rhythm
    
    // Crear osciladores est√©reo
    const leftOsc = audioContext.createOscillator()
    const rightOsc = audioContext.createOscillator()
    
    leftOsc.frequency.value = leftFreq
    rightOsc.frequency.value = rightFreq
    
    // Pan hard L/R
    const pannerL = new StereoPannerNode(audioContext, { pan: -1 })
    const pannerR = new StereoPannerNode(audioContext, { pan: 1 })
    
    leftOsc.connect(pannerL).connect(audioContext.destination)
    rightOsc.connect(pannerR).connect(audioContext.destination)
    
    leftOsc.start()
    rightOsc.start()
    
    return () => {
      leftOsc.stop()
      rightOsc.stop()
    }
  }, [coherence])
}
```

**Por qu√© AHORA**:
- Web Audio API nativa, no requiere librer√≠as
- Mejora experiencia meditativa inmediatamente
- Testeable con datos simulados
- Feature diferenciador (pocas apps lo tienen)

### 2.3 Shaders Cu√°nticos Avanzados

**Efectos a implementar**:

#### A. Quantum Collapse Effect
```glsl
// Part√≠culas en superposici√≥n ‚Üí colapso cuando coherencia sube
uniform float uCoherence;
uniform float uTime;

// Funci√≥n de ruido (simplex o Perlin)
float noise(vec3 p);

void main() {
    // Posici√≥n base
    vec3 basePos = vPosition;
    
    // Superposici√≥n cu√°ntica (part√≠culas flotando ca√≥ticamente)
    vec3 quantumNoise = vec3(
        noise(vPosition + uTime * 0.5),
        noise(vPosition + uTime * 0.7),
        noise(vPosition + uTime * 0.3)
    ) * 0.2;
    
    // Estado superpuesto
    vec3 superposedPos = basePos + quantumNoise;
    
    // Estado colapsado (ordenado)
    vec3 collapsedPos = basePos;
    
    // Mix basado en coherencia: 0.0 = caos, 1.0 = orden
    vec3 finalPos = mix(superposedPos, collapsedPos, uCoherence);
    
    gl_Position = projectionMatrix * modelViewMatrix * vec4(finalPos, 1.0);
}
```

#### B. Wave Function Visualization
```glsl
// Fragment shader: Ondas de probabilidad
uniform float uCoherence;
uniform vec3 uFocalPoint;

void main() {
    float dist = distance(vPosition, uFocalPoint);
    
    // Funci√≥n de onda: amplitud decae con distancia
    float waveAmplitude = exp(-dist * 2.0);
    
    // Frecuencia de onda aumenta con coherencia (m√°s "definida")
    float waveFreq = 5.0 + (uCoherence * 20.0);
    
    // Onda sinusoidal
    float wave = sin(dist * waveFreq - uTime * 3.0) * waveAmplitude;
    
    // Color: Azul fr√≠o (baja coherencia) ‚Üí Oro c√°lido (alta coherencia)
    vec3 coldColor = vec3(0.0, 0.5, 1.0);
    vec3 hotColor = vec3(1.0, 0.8, 0.2);
    vec3 baseColor = mix(coldColor, hotColor, uCoherence);
    
    // Intensidad modulada por onda
    float intensity = 0.5 + (wave * 0.5);
    vec3 finalColor = baseColor * intensity;
    
    gl_FragColor = vec4(finalColor, 0.8);
}
```

**Por qu√© AHORA**:
- Diferenciador visual √∫nico
- No depende de hardware
- Mejora la narrativa "colapso cu√°ntico = coherencia"
- F√°cil de testear con valores manuales

---

## üß™ FASE 3: DATASET & M√âTRICAS (2-3 d√≠as)

**Objetivo**: Mejorar calidad de inferencias con mejor dataset.

### 3.1 Dataset de Meditaci√≥n

**Opci√≥n 1: Kaggle EEG Meditation**
```bash
kaggle datasets download -d birdy654/eeg-brainwave-dataset-mental-state
```

**Opci√≥n 2: Synthetic Meditation Data**
```python
# backend/ai/data/synthetic_meditation.py
class MeditationSynthesizer:
    def generate_meditation_eeg(duration_seconds=60):
        """
        Genera EEG sint√©tico con caracter√≠sticas de meditaci√≥n:
        - Alpha dominante (8-12 Hz)
        - Alta coherencia inter-hemisf√©rica
        - Baja varianza temporal
        """
        # Usar se√±ales sinusoidales + ruido controlado
        pass
```

**Por qu√© AHORA**:
- Mejora realismo de las m√©tricas
- Permite fine-tuning del VAE
- No requiere hardware f√≠sico

### 3.2 M√©tricas de Validaci√≥n

```python
# backend/analysis/validation.py
class MetricsValidator:
    def validate_coherence_range(self, coherence):
        """Coherencia debe estar en [0, 1]"""
        assert 0 <= coherence <= 1
        
    def validate_focal_point_bounds(self, focal_point):
        """Focal point debe estar dentro del cerebro"""
        assert abs(focal_point['x']) < 2.0
        assert abs(focal_point['y']) < 2.0
        assert abs(focal_point['z']) < 2.0
        
    def validate_frequency_bands(self, bands):
        """Bandas deben sumar ~1.0 (normalizado)"""
        total = sum(bands.values())
        assert 0.9 < total < 1.1
```

**Por qu√© AHORA**:
- Garantiza calidad antes de integrar hardware
- Evita bugs silenciosos
- Profesionaliza el c√≥digo

---

## üéÆ FASE 4: NEUROFEEDBACK SIMULADO (3-4 d√≠as)

**Objetivo**: Sistema de entrenamiento funcional con datos simulados.

### 4.1 Modo Pr√°ctica

```jsx
// frontend/src/components/modes/PracticeMode.jsx
const PracticeMode = () => {
  const [target, setTarget] = useState(0.7) // Objetivo: 70% coherencia
  const { coherence } = useBrainStore()
  
  const progress = coherence / target
  
  return (
    <div>
      <h2>Practice: Reach 70% Coherence</h2>
      <ProgressBar value={progress} />
      
      {coherence >= target && (
        <Success>üéâ Target achieved! Hold for 30s...</Success>
      )}
      
      <Instructions>
        ‚Ä¢ Close your eyes
        ‚Ä¢ Focus on your breath
        ‚Ä¢ Visualize golden light
      </Instructions>
    </div>
  )
}
```

### 4.2 Sistema de Logros

```javascript
// frontend/src/store/achievementsStore.js
const achievements = [
  { id: 1, name: "First Contact", desc: "Reach 60% coherence", threshold: 0.6 },
  { id: 2, name: "Syntergic Flow", desc: "Maintain 70% for 1 min", threshold: 0.7, duration: 60 },
  { id: 3, name: "Unified Field", desc: "Reach 90% coherence", threshold: 0.9 },
]
```

**Por qu√© AHORA**:
- Gamificaci√≥n aumenta engagement
- Testeable con datos simulados (usuario puede "fingir" con botones de modo)
- Framework listo para cuando tengamos EEG real

---

## üìä FASE 5: OPTIMIZACI√ìN & POLISH (2-3 d√≠as)

### 5.1 Performance

**Tareas**:
- [ ] Instancing de part√≠culas en shaders (1 draw call)
- [ ] Web Workers para c√°lculos FFT
- [ ] Lazy loading de modelos 3D
- [ ] Memoization de componentes React

### 5.2 UX Profesional

**Tareas**:
- [ ] Loading states (skeleton screens)
- [ ] Error boundaries
- [ ] Modo offline (funciona sin backend)
- [ ] Tutorial interactivo first-time user

### 5.3 Documentaci√≥n

**Crear**:
- [ ] `SETUP.md` - C√≥mo correr el proyecto
- [ ] `API.md` - Documentaci√≥n de endpoints
- [ ] `THEORY.md` - Explicaci√≥n de m√©tricas sint√©rgicas
- [ ] Video demo (2-3 min)

---

## üî¨ FASE 6: CUANDO LLEGUE EL HARDWARE (1-2 d√≠as)

**M√≥dulo √∫nico a crear**:
```python
# backend/hardware/eeg_stream.py
class HardwareEEGStream:
    def __init__(self, device_type='muse'):
        # Conectar v√≠a LSL
        pass
    
    def get_next_sample(self):
        # Retornar array de EEG real
        pass
```

**Cambio en `inference.py`**:
```python
# ANTES (simulado)
real_eeg_input = next(self.iterators[self.current_mode])

# DESPU√âS (real)
if hardware_available:
    real_eeg_input = self.hardware_stream.get_next_sample()
else:
    real_eeg_input = next(self.iterators[self.current_mode])  # Fallback
```

**Tiempo estimado**: 1-2 d√≠as m√°ximo (porque TODO lo dem√°s ya est√° listo).

---

## üìÖ CRONOGRAMA OPTIMIZADO (Sin hardware)

### Semana 1: Backend Cient√≠fico
- **D√≠a 1-2**: M√≥dulo `analysis/` (spectral, coherence, entropy)
- **D√≠a 3**: Refactorizaci√≥n de `inference.py`
- **D√≠a 4**: Tests y validaci√≥n
- **D√≠a 5**: Dataset de meditaci√≥n

### Semana 2: Frontend Rico
- **D√≠a 1-2**: Componentes HUD (CoherenceMeter, FrequencySpectrum)
- **D√≠a 3**: Audio binaural reactivo
- **D√≠a 4-5**: Shaders cu√°nticos avanzados

### Semana 3: Experiencia Completa
- **D√≠a 1-2**: Modo pr√°ctica + logros
- **D√≠a 3**: Optimizaci√≥n performance
- **D√≠a 4**: UX polish + errores
- **D√≠a 5**: Documentaci√≥n + video demo

### Semana 4: INTEGRACI√ìN HARDWARE
- **D√≠a 1**: M√≥dulo `hardware/eeg_stream.py`
- **D√≠a 2**: Testing con EEG real
- **D√≠a 3+**: Calibraci√≥n y ajustes

---

## üéØ PRIORIDADES (Orden de Ejecuci√≥n)

### üî¥ CR√çTICO (Hacerlo primero)
1. **An√°lisis Espectral (FFT)** - Backend necesita enviar bandas de frecuencia
2. **CoherenceMeter Component** - Frontend necesita mostrar m√©tricas ricas
3. **Refactorizaci√≥n modular** - C√≥digo se est√° volviendo spagueti

### üü° IMPORTANTE (Segunda ola)
4. **Shaders cu√°nticos** - Diferenciador visual
5. **Audio binaural** - Feature √∫nico
6. **FrequencySpectrum component** - Completa el dashboard

### üü¢ NICE TO HAVE (Tercera ola)
7. **Modo pr√°ctica** - Gamificaci√≥n
8. **Dataset meditaci√≥n** - Mejor realismo
9. **Optimizaciones** - Performance

---

## üö´ LO QUE **NO** HAREMOS HASTA TENER HARDWARE

- ‚ùå Integraci√≥n LSL (Lab Streaming Layer)
- ‚ùå Calibraci√≥n de electrodos
- ‚ùå Detecci√≥n de artefactos hardware-espec√≠fica
- ‚ùå Filtrado hardware en tiempo real
- ‚ùå UI de configuraci√≥n de dispositivo

---

## ‚úÖ DEFINITION OF DONE (Cada fase)

### Backend
- [ ] Tests pasan (pytest)
- [ ] M√©tricas validadas con asserts
- [ ] C√≥digo documentado (docstrings)
- [ ] Sin warnings en consola

### Frontend
- [ ] 60 FPS constante
- [ ] Funciona sin backend (modo demo)
- [ ] Responsive (desktop + tablet)
- [ ] Accesibilidad b√°sica (keyboard navigation)

---

## üé¨ PRIMER PASO (Hoy mismo)

### Backend: An√°lisis Espectral

```python
# backend/analysis/spectral.py
import numpy as np
from scipy import signal
from scipy.fft import fft, fftfreq

class SpectralAnalyzer:
    @staticmethod
    def compute_frequency_bands(eeg_signal: np.ndarray, fs: int = 256) -> dict:
        """
        Calcula potencia en bandas de frecuencia est√°ndar.
        
        Args:
            eeg_signal: Array 1D de EEG
            fs: Frecuencia de muestreo
            
        Returns:
            dict: {'delta': float, 'theta': float, 'alpha': float, 'beta': float, 'gamma': float}
        """
        # FFT
        N = len(eeg_signal)
        yf = fft(eeg_signal)
        xf = fftfreq(N, 1/fs)[:N//2]
        
        # Power Spectral Density
        psd = 2.0/N * np.abs(yf[0:N//2])
        
        # Definir bandas
        bands = {
            'delta': (0.5, 4),
            'theta': (4, 8),
            'alpha': (8, 13),
            'beta': (13, 30),
            'gamma': (30, 50)
        }
        
        # Calcular potencia por banda
        band_powers = {}
        for band_name, (low, high) in bands.items():
            idx_band = np.logical_and(xf >= low, xf <= high)
            band_powers[band_name] = np.mean(psd[idx_band])
        
        # Normalizar a [0, 1]
        total_power = sum(band_powers.values())
        return {k: v/total_power for k, v in band_powers.items()}
```

**Integrar en `main.py`**:
```python
from analysis.spectral import SpectralAnalyzer

# En websocket_endpoint:
ai_state = brain.next_state()

# NUEVO: Calcular bandas de frecuencia
bands = SpectralAnalyzer.compute_frequency_bands(
    ai_state['raw_eeg'],  # ‚Üê Necesitamos pasar EEG crudo
    fs=256
)

state = SyntergicState(
    timestamp=current_t,
    coherence=ai_state["coherence"],
    entropy=ai_state["entropy"],
    focal_point=ai_state["focal_point"],
    frequency=10.0 + (ai_state["coherence"] * 30.0),
    bands=bands  # ‚Üê NUEVO campo
)
```

**Actualizar `models.py`**:
```python
class SyntergicState(BaseModel):
    timestamp: float
    coherence: float
    entropy: float
    focal_point: dict
    frequency: float
    bands: dict = None  # ‚Üê NUEVO: {'delta': 0.1, 'theta': 0.2, ...}
```

---

## üéì CONCLUSI√ìN

**Estrategia**:
1. **Perfeccionar backend** (m√©tricas cient√≠ficas reales)
2. **Enriquecer frontend** (visualizaciones + audio)
3. **Gamificar experiencia** (pr√°ctica + logros)
4. **Cuando llegue hardware**: Solo conectar fuente de datos

**Ventajas**:
- Sistema 100% funcional sin esperar hardware
- Demostrable a inversores/colaboradores
- Testing exhaustivo con datos controlados
- Integraci√≥n hardware ser√° trivial (1-2 d√≠as)

**Meta**: En 3 semanas tener un producto COMPLETO que funcione con simulaci√≥n. Cuando llegue el EEG, ser√° solo "enchufar y listo".

---

**¬øEmpezamos con el m√≥dulo de an√°lisis espectral (backend) o prefieres atacar los shaders cu√°nticos (frontend) primero?**
